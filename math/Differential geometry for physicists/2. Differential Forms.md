II. Differential Forms
===



## 1-Forms



#### Definition : 1-form

$\phi : T_p (\mathbb{R}^n)\to \mathbb{R}$을 **1-form** at $p$ 라 한다. 다른 말로 하면, 1-form은 linear functional of $T_p(\mathbb{R}^n)$ 이다. 따라서 다음을 만족한다.
$$
\phi (aX_p + bY_p) = a\phi(X_p)+b\phi (Y_p) \qquad \forall a,\,b \in \mathbb{R},\;\forall X_p,\,Y_p \in T_p(\mathbb{R}^n)\;.
$$




#### Definition : Differential of smooth function.

$f \in \mathscr{F}(\mathbb{R}^\infty)$ 일 때 **differential** of $f$, $df$를 다음과 같은 $1$-form 으로 정의한다.
$$
df(X)=X(f)\;.
$$
즉, $\mathbf{p}\in \mathbb{R}^n$ 일 때 $df(X)(p)=X_p (f) = \nabla f_p \cdot \mathbf{X}(p)$ 이다.



#### Lemma 1.1

 Coordinate function $x^i$ 에 대해 $dx^i(\dfrac{\partial}{\partial x^j})= \dfrac{\partial x^i}{\partial x^j}= \delta^i_j$ 이다.

---

*Proof is trivial*



#### Definition : Dual of the tangent space

우리는 어떤 vector space $V$에 대한 모든 linear functional의 집합이 vector space 임을 알고 있다. 이 linear functional의 집합을 **dual** of $V$ 라 하며 $T_p (\mathbb{R}^n)$ 의 dual을 $T^*_p (\mathbb{R}^n)$ 을 **cotangent space** of $\mathbb{R}^n$ at $\mathbb{p}$ 라 한다. 



#### Lemma 1.2

$T_p (\mathbb{R}^n)$ 의 basis를 $\left\{ \left( \dfrac{\partial}{\partial x^1} \right)_p,\ldots,\,\left( \dfrac{\partial}{\partial x^n} \right)_p \right\}$ 라 할 때 $\{(dx^1)_p,\ldots,\,(dx^n)_p\}$ 는 이에 대한 $T^*_p (\mathbb{R}^n)$ 의 basis 이다.

---

*proof is trivial*



#### Theorem 1.3

$f\in \mathscr{F}(\mathbb{R}^n)$ 이고 $\{x^1,\ldots,\,x^n\}$ 이 $\mathbb{p}$의 neighborhood $U$의 coordinate functions 일 때 $df$는 다음과 같다.
$$
df = \sum_{i=1}^n \dfrac{\partial f}{\partial x^i} dx^i = \left(\dfrac{\partial f}{\partial x^i}\right)dx^i \;.
$$

---

*(Proof)* $X_p = v^j \left(\dfrac{\partial}{\partial x^j}\right)$ 임을 이용하면, 
$$
\begin{align*}
\left( \dfrac{\partial f}{\partial x^i} dx^i\right)_p (X_p) &= \left(\dfrac{\partial f}{\partial x^i} dx^i\right)\left( v^j \dfrac{\partial }{\partial x^j}\right)(p) \\
&= v^j \left(\dfrac{\partial f}{\partial x^i} dx^i\right)\left( \dfrac{\partial }{\partial x^j}\right)(p) \\
&= v^j \dfrac{\partial f}{\partial x^i}\dfrac{\partial x^i}{\partial x^j}= v^j \dfrac{\partial f}{\partial x^i}\delta^i_j (p) \\
&= \left(\dfrac{\partial f}{\partial x_i}v^i \right) (p)= \nabla f(p) \cdot \mathbf{x}(p)=df(X)(p) \qquad \square

\end{align*}
$$


<b>Note : </b> The definition of differentials as linear functionals on the space of vector fields is much more satisfactory than the notion of infinitesimals, since the new definition is based on the rigorous machinery of linear algebra.



#### Definition : Cotangent tensor, covector, and exact form.

Let $\alpha = a_1 (\mathbf{x})dx^1 + \cdots + a_n (\mathbf{x})dx^n$  where $a_i\in \mathscr{F}(U)$ for some open $U$ containing $\mathbf{p}\in \mathbb{R}^n$. $\alpha$는 $1$-form 이며 이러한 $\alpha$를 **cotangent tensor** of rank $1$, 혹은 **covector** 라 한다. 모든 $1$-form 을 $\mathscr{T}_1^0 (\mathbb{R}^n)$ 이라 쓴다. 그리고 이 때의 $(a_1,\ldots,\,a_n)$ 을 **covariant** components of covector $\alpha$ 라 한다. 어떤 $f\in \mathscr{F}(\mathbb{R}^n)$ 에 대해 $\alpha = df$ 일 때 $\alpha$를 **exact** 라 한다.



#### Lemma 1.4

$\alpha,\,\beta \in \mathscr{T}^0_1 (\mathbb{R}^n)$ 이고 $f \in \mathscr{F}(\mathbb{R}^n)$ 일 때 모든 vector field $X$에 대해 다음이 성립한다.
$$
\begin{align*}
(\alpha +\beta)(X) & = \alpha (X)+ \beta (X) \;,\\
(f\alpha)(X) &= f\cdot \alpha(X)\;.
\end{align*}
$$

---

*Proof is trivial*



## 2. Tensors and Forms of Higher Rank



#### Definition : Bilinear map vector field

$\phi : \mathscr{K} (\mathbb{R}^n) \times \mathscr{K}(\mathbb{R}^n) \to \mathbb{R}$ 이 모든 $X_i,\,Y_i \in \mathscr{K}(\mathbb{R}^n)$ 과 $f^i \in \mathscr{F}(\mathbb{R}^n)$ 에 대해 다음을 만족할 때 이를 **bilinear map** 이라 한다.
$$
\begin{align*}
\phi (f^1 X_1 + f^2 X_2,\,Y_1) &= f^1 \phi (X_1,\,Y_1)+ f^2\phi (X_2,\,Y_1)\;,\\
\phi (X_1,\,f^1Y_1 + f^2 Y_2)&= f^1 \phi (X_1,\,Y_1) + f^2 \phi (X_1,\,Y_2)\;.
\end{align*}
$$


#### Definition : Tensor product

$\alpha,\,\beta \in \mathscr{T}^0_1 (\mathbb{R}^n)$ 일 때 다음과 같이 정의된 bilinear map $\alpha \otimes \beta : \mathscr{K}(\mathbb{R}^n) \times \mathscr{K}(\mathbb{R}^n)\to \mathbb{R}$을 **tensor product** of $\alpha$ and $\beta$라 한다.
$$
\alpha \otimes \beta (X,\,Y) = \alpha(X)\beta(Y)\;.
$$


$\alpha = a_i dx^i$, $\beta = b_j dx^j$ 일 때,
$$
\begin{align*}
\alpha \otimes \beta (\dfrac{\partial }{\partial x^k},\,\dfrac{\partial}{\partial x^l}) &= \alpha (\dfrac{\partial}{\partial x^k})\beta (\dfrac{\partial}{\partial x^l}) = a_i \delta^i_k b_j \delta^j_l=a_kb_l\;
\end{align*}
$$
이다. 



#### Definition : Covariant tensor of rank 2

$T=T_{ij}dx^i \otimes dx^j$ 꼴로 정의된 $T$를 **covariant tensor of rank 2** 라 하며 $\mathbb{R}^n$에서 정의된 모든 covariant tensor fields of rank-$2$의 집합을 $\mathscr{T}^0_2(\mathbb{R}^n)$ 이라 쓴다. 이 때 우리는 $\{dx^i \otimes dx^j\}$ 를 $\mathscr{T}^0_2 (\mathbb{R}^n)$ 의 basis로 생각 할 수 있다.



#### Definition : Tensor product of Vectors

위의 Covariant rank 2 tensor의 정의로부터 임의의 smooth function $f,\,g$에 대한 bilinear map by tensor product 를 다음과 같이 정의 할 수 있다.
$$
(X \otimes Y)(f,\,g)=X(f)Y(g)\;.
$$
(1) Let $X=a^i \dfrac{\partial }{\partial x^i},\,Y=b^j \dfrac{\partial}{\partial x^j}$ .  Then $T=T^{ij}\dfrac{\partial}{\partial x^i} \otimes  \dfrac{\partial}{\partial x^j}$ 꼴인 tensor를 **contravariant tensor of rank 2** 라 하며 $\mathbb{R}^n$에서의 contravariant tensor of rank 2의 집합을 $\mathscr{T}^2_0 (\mathbb{R}^n)$ 이라 한다.

(2) $T=T^{ij}_k \dfrac{\partial }{\partial x_i}\otimes \dfrac{\partial}{\partial x^j}\otimes dx^k$ 와 같이 contravariant와 covariant가 섞인 higher rank tensor를 생각 할 수 있으며 이때 $\mathscr{T}^n_m (\mathbb{R}^n)$ 을 생각 할 수 있을 것이다.



#### Definition : Inner product, metric

Bilinear map of vector fields $g (X,\,Y)$가 모든 $X,\,Y \in \mathscr{K}(\mathbb{R}^n$에 대해 다음을 만족하면 **inner product** 라 한다.

1. $g(X,\,Y)= g(Y,\,X)$.
2. $g(X,\,X)\ge 0$ 
3. $g(X,\,X)=0  \iff X=0$. 

이 때 $g \left( \dfrac{\partial}{\partial x^i},\, \dfrac{\partial}{\partial x^j}\right)=g_{ij}$ 로 정의하며 이를 **metric** 이라 한다. $g_{ij}$ 는 symmetric $n\times n$ 행렬로 **metric tensor** 라 하며 non-singular 하다. $X=a^i \dfrac{\partial }{\partial x^i},\, Y=b^j \dfrac{\partial }{\partial x^j}$ 이면 $g(X,\,Y)= g_{ij}a^ib^j$ 이다.



#### Lemma 2.1

$g_{ij}= \delta_{ij}$ 이면 우리가 보통 사용하는 dot product 이다.

---

*Proof is trivial*



#### Dot product revisited

$X=a^i \dfrac{\partial}{\partial x^i}$ , $\alpha = b_j dx^j$ 라 하자. 그렇다면
$$
\alpha (X) = b_j dx^j (a^i \dfrac{\partial}{\partial x^i})=a^ib_j \delta^j_i = a^i b_i
$$
이다. 이 때 $b_i$를 $b_i = g_{ij}b^j$ 로 정의하면 $a^ib_j = g_{ij}a^ib^j$ 이다. 우리는 이렇게 metric이 covariant components of vector와 contravariant components of vector 사이의 변환에 사용될 수 있음을 알게 되었다. $g^{ij}$ 를 $g_{ij}$ 의 역행렬의 components라 하면,
$$
g^{ik}g_{kj}=\delta^i_j
$$
이다.



