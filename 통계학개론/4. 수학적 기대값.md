IV. 수학적 기대값
===



## 1. 확률변수의 평균



#### 확률변수 $X$의 평균, 기대값

확률변수 $X$의 평균값을 $\mu_X$ 로 표기하며 간단히  $\mu$로 표기하기도 한다. 혹은 $X$의 **기대값** 혹은 **수학적 기대값**이라 하고 $E(X)$ 로 표기한다. $X$ 에 대한 확률분포를 $f(x)$라 할 때 수학적으로는 다음과 같다.
$$
\begin{align*}
\mu =E(X)=\sum_x xf(x)  \quad\text{or}\quad \mu = E(X) = \int_{-\infty}^\infty xf(x)\, dx\;.
\end{align*}
$$


#### 새로운 확률변수

$X$에 종속된 새로운 확률 변수 $g(X)$ 를 생각하자. 이 때 $g(X)$의 기대값은 다음과 같다.
$$
\mu_{g(X)}=E[g(X)]=\sum_x g(x)f(x)\quad \text{or}\quad \mu_{g(X)} =E[g(x)]=\int_{-\infty}^\infty g(x)f(x)\,dx\;.
$$
확률변수 $X,\,Y$의 확률분포가 $f(x,\,y)$ 이고 $X,\,Y$에 종속된 새로운 확률 변수 $g(X,\,Y)$ 를 생각 할 수 있다. 이 때 $g(X,\,Y)$의 기대값은 다음과 같다.
$$
\mu_{g(X,\,Y)}=E[g(X,\,Y)]=\sum_{x}\sum_{y} g(x,\,y)f(x,\,y)\quad\text{or}\quad \int_{-\infty}^\infty \int_{-\infty}^\infty g(x,\,y)f(x,\,y)\, dxdy\;.
$$


## 2. 분산과 공분산



#### 분산, 표준편차.

$X$에 대한 확률분포 $f(X)$가 존재할 때 $(X-\mu)^2$ 의 기대값을 **확률변수 $X$의 분산 (variance of the random variable $X$)** 라 하고 $\operatorname{Var}(X)$ 혹은 $\sigma_X^2$ 로 표기한다. 분산의 양의 제곱근을 **표준편차 (standard deviation)** 이라 하며 $\sigma_X$ 로 표기한다.



#### 확률분포의 분산

확률변수 $X$ 그 확률분포 $f(x)$ 가 있다고 하자.  그 기대값 $\mu$ 에 대해 다음이 성립한다.
$$
\sigma^2=E(X^2)-\mu^2
$$

---

*(proof)*
$$
\begin{align*}
\sigma^2 &= \sum_{x}(x-\mu)^2 f(x) = \sum_x (x^2-2\mu x+\mu^2)f(x) \\
&=\sum_{x} x^2 f(x) - 2\mu\sum_x xf(x) + \mu^2\sum_x f(x) \\
&=E(X^2)-2\mu^2+\mu^2 = E(X^2)-\mu^2 \qquad\square

\end{align*}
$$


$X$에 의존하는 새로운 확률변수 $g(X)$에 대해 $g(X)$의 분산은 다음과 같다.
$$
\sigma_{g(X)}^2 = E[(g(X)-\mu_{g(X)})^2]=\sum_x(g(x)-\mu_{g(X)})^2f(x) \qquad\text{or}\qquad \int_{-\infty}^\infty \left[g(x)-\mu_{g(X)}\right]^2f(x)\, dx\,.
$$


#### 공분산

확률변수 $X,\,Y$ 와 확률분포 $f(X,\,Y)$ 에 대해 $\mu_X=E(X),\,\mu_Y=E(Y)$ 에 대해 $g(X,\,Y)=(X-\mu_X)(Y-\mu_Y)$ 의 기대값을  **공분산 (covariance)** 이라 하고 $\sigma_{XY}$ 라 쓴다 이 때 다음 식이 성립한다.
$$
\sigma_{XY}=E(XY)-\mu_X\cdot\mu_Y\;.
$$

---

*(proof)*
$$
\begin{align*}
\sigma_{XY}&= \sum_x\sum_y (x-\mu_X)(y-\mu_Y)f(x,\,y) \\
&= \sum_x\sum_y xy f(x,\,y) -\mu_Y\sum_x\sum_y xf(x,\,y)-\mu_X \sum_x\sum_y yf(x,\,y)+ \mu_X \mu_Y \sum_x \sum_y f(x,\,y) \\
&=E(XY)-\mu_X\mu_Y-\mu_X\mu_Y+\mu_X\mu_Y = E(XY)-\mu_X\mu_Y\;. \qquad\square

\end{align*}
$$




#### 상관계수

확률변수 $X,\,Y$ 의 공분산이 $\sigma_{XY}$ 이고 표준편차가 각각 $\sigma_X,\,\sigma_Y$ 일 때 다음과 같이 정의된 $\rho_{XY}$ 를 **상관계수 (correlation coefficient)** 라 한다.
$$
\rho_{XY}= \dfrac{\sigma_{XY}}{\sigma_X \sigma_Y}\;.
$$
이 때 $-1\le \rho_{XY}\le 1$ 이 항상 성립한다. (증명은 나중에)



## 3. 선형결합된 확률변수의 평균과 분산



#### 선형결합된 확률변수의 평균

확률변수 $X,\,Y$, 상수 $a,\,b$에 대해 다음이 성립한다.

- $E(aX+b)=aE(X)+b$,
- $E[g(X)\pm h(X)]=E[g(X)]\pm E[h(X)]$,
- $E[g(X,\,Y)\pm h(X,\,Y)] = E[g(X,\,Y)]\pm E[h(X,\,Y)]$,
- $E[g(X)\pm h(Y)]=E[g(X)]\pm E[h(y)]$



#### 독립인 두 변수의 평균과 공분산

두 확률변수 $X,\,Y$ 가 독립이면 $E(XY)=E(X)E(Y)$ 이며 $\sigma_{XY}=0$ 이다.

---

*proof is trivial*



#### 선형결합인 두 확률변수의 분산

$X,\,Y$가 결합확률분포 $f(x,\,y)$를 가지는 확률변수이고 $a,\,b,\,c$가 상수일 때 다음이 성립한다.
$$
\sigma^2_{aX+bY+c}=a^2\sigma^2_X+ b^2\sigma_Y^2+2ab\sigma_{XY}\;.
$$

---

*(proof)* 
$$
\begin{align*}
\sigma_{aX+bY+c}^2 &= E[(aX+bY+c-\mu_{aX+bY+c})^2] =E[(aX+bY+c-a\mu_X-b\mu_Y-c)^2] \\
&=E[\{a(X-\mu_x)+b(Y-\mu_Y) \}^2] \\
&=a^2E[(X-\mu)^2]+b^2E[(Y-\mu_Y)^2]+2abE[(X-\mu_X)(Y-\mu_Y)] \\
&= a^2\sigma_X^2+b^2\sigma_Y^2+2ab\sigma_{XY} \;.\qquad\square

\end{align*}
$$


만약 $X,\,Y$ 가 서로 독립이면 $\sigma_{XY}=0$ 이므로 $\sigma^2_{aX+bY+c}=a^2\sigma_X^2+b^2\sigma_Y^2$ 이다. 또한 $X_1,\ldots,\,X_n$ 이 독립인 확률변수라 하면 $\sigma^2_{a_1X_1 + \cdots + a_n X_n}= a_1^2 \sigma_{X_1}^2 + \cdots + a_n^2 \sigma_{X_n}^2$ 이다. 



## 4. 체비셰프 정리



#### 체비셰프 정리

확률변수 $X$가 평균으로부터 표준편자의 $k$배 범위 내의 값을 취할 확률은 적어도 $1-1/k^2$ 이다. 즉 
$$
P(\mu-k\sigma_X<X<\mu+k\sigma_X) \ge 1-\dfrac{1}{k^2}
$$
이다.

---

*(proof)* 분산의 정의에 의해,
$$
\begin{align*}
\sigma^2 &= E[(X-\mu)^2]=\sum_x (x-\mu)^2f(x) \\
&= \sum_{x\le\mu-k\sigma} (x-\mu)^2f(x) + \sum_{\mu-k\sigma<x<\mu+k\sigma} (x-\mu)^2f(x) + \sum_{x\ge\mu+k\sigma} (x-\mu)^2f(x) \\
&\ge \sum_{x\le\mu-k\sigma} (x-\mu)^2f(x) +  \sum_{x\ge\mu+k\sigma} (x-\mu)^2f(x)
\end{align*}
$$
이다. 그런데 $x\le \mu-k\sigma$ 인 경우나 $x\ge \mu+k\sigma$ 인 경우 모두 $(x-\mu)^2\ge k^2\sigma^2$ 이므로,
$$
\begin{align*}
\sigma^2 \ge k^2\sigma^2 \sum_{|x-\mu|\ge k\sigma} f(x) \implies P(X\le \mu-k\sigma)+P(X\ge \mu+k\sigma) \le \dfrac{1}{k^2}
\end{align*}
$$
따라서, 체비셰프 정리가 성립한다.$\qquad\square$





